@misc{athreya_hole_2025,
 abstract = {Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces {HOLE} (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. {HOLE} extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate {HOLE} on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.},
 author = {Athreya, Sudhanva Manjunath and Rosen, Paul},
 date = {2025-12-10},
 doi = {10.48550/arXiv.2512.07988},
 eprint = {2512.07988 [cs]},
 eprinttype = {arxiv},
 keywords = {1 - Visualization, 1 - machine learning:deep learning, 1 - machine learning:explainable {AI}, 1 - machine learning:neural network, 2 - persistent homology:dendrograms, 2 - persistent homology:persistence diagrams, 3 - images, 3 - neural networks:activation, 3 - point cloud, C - https://github.com/{FoxHound}0x00/hole, https://github.com/{FoxHound}0x00/hole-dashboard},
 number = {{arXiv}:2512.07988},
 publisher = {{arXiv}},
 shorttitle = {{HOLE}},
 title = {{HOLE}: Homological Observation of Latent Embeddings for Neural Network Interpretability},
 url = {http://arxiv.org/abs/2512.07988},
 urldate = {2026-01-27}
}
