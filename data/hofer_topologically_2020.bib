@inproceedings{hofer_topologically_2020,
 abstract = {We study regularization in the context of small sample-size learning with over-parametrized neural networks. Specifically, we shift focus from architectural properties, such as norms on the network weights, to properties of the internal representations before a linear classifier. Specifically, we impose a topological constraint on samples drawn from the probability measure induced in that space. This provably leads to mass concentration effects around the representations of training instances, i.e., a property beneficial for generalization. By leveraging previous work to impose topological constrains in a neural network setting, we provide empirical evidence (across various vision benchmarks) to support our claim for better generalization.},
 author = {Hofer, Christoph and Graf, Florian and Niethammer, Marc and Kwitt, Roland},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning},
 date = {2020-11-21},
 eventtitle = {International Conference on Machine Learning},
 keywords = {1 - computer science:machine learning:neural network, 2 - Persistent homology:Vietoris-rips filtration, 2 - probability measure mass concentration, 2 - topological regularization of latent space, 3 - Embeddings:Probability distributions, Innovate, S - https://câ€‘hofer.github.io/torchph/},
 langid = {english},
 pages = {4304--4313},
 publisher = {{PMLR}},
 title = {Topologically Densified Distributions},
 url = {https://proceedings.mlr.press/v119/hofer20a.html},
 urldate = {2022-10-12}
}
